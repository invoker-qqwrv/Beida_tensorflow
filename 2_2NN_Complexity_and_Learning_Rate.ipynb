{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsW71193/DySaJM10tTKGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invoker-qqwrv/Beida_tensorflow/blob/main/2_2NN_Complexity_and_Learning_Rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KSn5H3bUIxz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f63794-1a76-4322-fad2-ec40fcb1adcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 0 epoch,w is 2.600000,loss is 36.000000,lr is 0.200000\n",
            "After 1 epoch,w is 1.174400,loss is 12.959999,lr is 0.198000\n",
            "After 2 epoch,w is 0.321948,loss is 4.728015,lr is 0.196020\n",
            "After 3 epoch,w is -0.191126,loss is 1.747547,lr is 0.194060\n",
            "After 4 epoch,w is -0.501926,loss is 0.654277,lr is 0.192119\n",
            "After 5 epoch,w is -0.691392,loss is 0.248077,lr is 0.190198\n",
            "After 6 epoch,w is -0.807611,loss is 0.095239,lr is 0.188296\n",
            "After 7 epoch,w is -0.879339,loss is 0.037014,lr is 0.186413\n",
            "After 8 epoch,w is -0.923874,loss is 0.014559,lr is 0.184549\n",
            "After 9 epoch,w is -0.951691,loss is 0.005795,lr is 0.182703\n",
            "After 10 epoch,w is -0.969167,loss is 0.002334,lr is 0.180876\n",
            "After 11 epoch,w is -0.980209,loss is 0.000951,lr is 0.179068\n",
            "After 12 epoch,w is -0.987226,loss is 0.000392,lr is 0.177277\n",
            "After 13 epoch,w is -0.991710,loss is 0.000163,lr is 0.175504\n",
            "After 14 epoch,w is -0.994591,loss is 0.000069,lr is 0.173749\n",
            "After 15 epoch,w is -0.996452,loss is 0.000029,lr is 0.172012\n",
            "After 16 epoch,w is -0.997660,loss is 0.000013,lr is 0.170292\n",
            "After 17 epoch,w is -0.998449,loss is 0.000005,lr is 0.168589\n",
            "After 18 epoch,w is -0.998967,loss is 0.000002,lr is 0.166903\n",
            "After 19 epoch,w is -0.999308,loss is 0.000001,lr is 0.165234\n",
            "After 20 epoch,w is -0.999535,loss is 0.000000,lr is 0.163581\n",
            "After 21 epoch,w is -0.999685,loss is 0.000000,lr is 0.161946\n",
            "After 22 epoch,w is -0.999786,loss is 0.000000,lr is 0.160326\n",
            "After 23 epoch,w is -0.999854,loss is 0.000000,lr is 0.158723\n",
            "After 24 epoch,w is -0.999900,loss is 0.000000,lr is 0.157136\n",
            "After 25 epoch,w is -0.999931,loss is 0.000000,lr is 0.155564\n",
            "After 26 epoch,w is -0.999952,loss is 0.000000,lr is 0.154009\n",
            "After 27 epoch,w is -0.999967,loss is 0.000000,lr is 0.152469\n",
            "After 28 epoch,w is -0.999977,loss is 0.000000,lr is 0.150944\n",
            "After 29 epoch,w is -0.999984,loss is 0.000000,lr is 0.149434\n",
            "After 30 epoch,w is -0.999989,loss is 0.000000,lr is 0.147940\n",
            "After 31 epoch,w is -0.999992,loss is 0.000000,lr is 0.146461\n",
            "After 32 epoch,w is -0.999994,loss is 0.000000,lr is 0.144996\n",
            "After 33 epoch,w is -0.999996,loss is 0.000000,lr is 0.143546\n",
            "After 34 epoch,w is -0.999997,loss is 0.000000,lr is 0.142111\n",
            "After 35 epoch,w is -0.999998,loss is 0.000000,lr is 0.140690\n",
            "After 36 epoch,w is -0.999999,loss is 0.000000,lr is 0.139283\n",
            "After 37 epoch,w is -0.999999,loss is 0.000000,lr is 0.137890\n",
            "After 38 epoch,w is -0.999999,loss is 0.000000,lr is 0.136511\n",
            "After 39 epoch,w is -0.999999,loss is 0.000000,lr is 0.135146\n"
          ]
        }
      ],
      "source": [
        "# 指数衰减学习率=初始学习率*学习率衰减率（当前轮数/多少轮衰减一次）一般写在for循环上\n",
        "# 原理是先用较大的学习率，快速得到最优解，然后逐渐减小学习率\n",
        "import tensorflow as tf\n",
        "w=tf.Variable(tf.constant(5, dtype=tf.float32))\n",
        "\n",
        "epoch=40\n",
        "LR_BASE=0.2  # 最初学习率\n",
        "LR_DECAY=0.99  # 学习率衰减率\n",
        "LR_STEP=1  # 喂入多少轮BATCH_SIZE后，更新一次学习率\n",
        "for epoch in range(epoch):\n",
        "  lr=LR_BASE*LR_DECAY**(epoch/LR_STEP)\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss=tf.square(w+1)\n",
        "  grads=tape.gradient(loss,w)\n",
        "  w.assign_sub(lr*grads)\n",
        "  print(\"After %s epoch,w is %f,loss is %f,lr is %f\" % (epoch, w.numpy(), loss, lr))\n",
        "  # %d表示int %f表示float  %s是str"
      ]
    }
  ]
}