{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFOdlMyLrbKKMOJBsarX68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invoker-qqwrv/Beida_tensorflow/blob/main/2_5_Alleviate_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiYuqdlKj5WK"
      },
      "outputs": [],
      "source": [
        "# 正则化缓解过拟合。在损失函数中引入模型复杂度指标，给w加权值，弱化数据的噪声\n",
        "# loss=loss（y与y_）+regularizer*loss（w）\n",
        "# L1正则化，大概率使很多参数变为0，所以该方法可以通过稀疏参数，即减少参数的数量，降低复杂度\n",
        "# L2正则化，使参数接近0但不等于0.所以该参数可以通过减小参数值的大小降低复杂度。\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "#读取数据，生成x_train和y_train\n",
        "# 读入数据/标签生成x_trainy_train\n",
        "df=pd.read_csv('dot.csv')\n",
        "x_data=np.array(df[['x1','x2']])\n",
        "y_data=np.array(df['y_c'])\n",
        "\n",
        "x_train=x_data\n",
        "y_train=y_data.reshape(-1,1)#只想让y_train变成一列。行数是多少不知道\n",
        "# https://blog.csdn.net/wld914674505/article/details/80460042 关于reshape函数的讲解\n",
        "Y_c=[['red' if y else 'blue'] for y in y_train]\n",
        "\n",
        "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型问题报错\n",
        "x_train=tf.cast(x_train,tf.float32)\n",
        "y_train=tf.cast(y_train,tf.float32)\n",
        "\n",
        "# from_tensor_slices函数切分传入的张量的第一个维度，生成相应的数据集，使输入特征和标签值一一对应\n",
        "train_db= tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)\n",
        "# 生成神经网络的参数，输入层为4个神经元，隐藏层为32个神经元，2层隐藏层，输出层为3个神经元\n",
        "# 用tf.Variable()保证参数可训练\n",
        "w1=tf.Variable(tf.random.normal([2,11]), dtype=tf.float32)#第一层网络输出是11个神经元\n",
        "b1=tf.Variable(tf.constant(0.01,shape=[11]))\n",
        "\n",
        "w2=tf.Variable(tf.random.normal([11,1]),dtype=tf.float32)\n",
        "b2=tf.Variable(tf.constant(0.01,shape=[1]))\n",
        "for epoch in range(epoch):\n",
        "  for step, (x_train,y_train) in enumerate(train_db):\n",
        "    with tf.GradientTape() as tape:  # 记录梯度信息\n",
        "       h1 = tf.matmul(x_train,w1)+b1  # 记录神经网络乘加运算\n",
        "       h1 = tf.nn.relu(h1)\n",
        "       y = tf.matmul(h1,w2)+b2"
      ]
    }
  ]
}