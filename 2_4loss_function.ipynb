{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRj2/KWsjvHImodkNmRpzf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invoker-qqwrv/Beida_tensorflow/blob/main/2_4loss_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyTaRGw2TpzW"
      },
      "outputs": [],
      "source": [
        "# 损失函数就是预测值y与标准值y_之间的差距。\n",
        "# mse：loss_mse=tf.reduce_mean(tf.square(y,y_))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x1和x2是影响日销量的因素。销量是y_。噪声-0.05-0.05\n",
        "# 这里用了mes，是默认了销量预测多了和少了，损失是一样的\n",
        "# 接下来构建一层的神经网络\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "SEED=23455\n",
        "rdm=np.random.RandomState(seed=SEED)  #生成[0,1)之间的随机数\n",
        "x=rdm.rand(32,2)#生成32行两列的随机数，也就是x1和x2\n",
        "y_=[[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in x]  # .rand函数生成噪声[0,1)/10=[0,0.1);[0,0.1)-0.05=[-0.05,0.05)\n",
        "x=tf.cast(x, dtype=tf.float32)\n",
        "w1=tf.Variable(tf.random.normal([2,1], stddev=1,seed=1))#随机初始化w1\n",
        "epoch=15000\n",
        "lr=0.002\n",
        "for epoch in range(epoch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y=tf.matmul(x,w1)#前向传播计算结果\n",
        "    loss_mse=tf.reduce_mean(tf.square(y_-y))#损失函数均方误差\n",
        "  grads=tape.gradient(loss_mse,w1)\n",
        "  w1.assign_sub(lr*grads)\n",
        "  if epoch%500==0:\n",
        "    print(\"After %d training steps,w1 is\"%(epoch))\n",
        "    print(w1.numpy(),\"\\n\")\n",
        "print(\"Final w1 is:\",w1.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLRNIpj9VUU6",
        "outputId": "cfbf85d1-6e2a-4a53-b658-da8584eec642"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 0 training steps,w1 is\n",
            "[[-0.7648834]\n",
            " [-0.4465386]] \n",
            "\n",
            "After 500 training steps,w1 is\n",
            "[[0.27169284]\n",
            " [0.63153684]] \n",
            "\n",
            "After 1000 training steps,w1 is\n",
            "[[0.64486957]\n",
            " [0.97207904]] \n",
            "\n",
            "After 1500 training steps,w1 is\n",
            "[[0.79063535]\n",
            " [1.0686706 ]] \n",
            "\n",
            "After 2000 training steps,w1 is\n",
            "[[0.85631734]\n",
            " [1.0864854 ]] \n",
            "\n",
            "After 2500 training steps,w1 is\n",
            "[[0.8920836]\n",
            " [1.0804592]] \n",
            "\n",
            "After 3000 training steps,w1 is\n",
            "[[0.91533583]\n",
            " [1.0685775 ]] \n",
            "\n",
            "After 3500 training steps,w1 is\n",
            "[[0.9323635]\n",
            " [1.0565025]] \n",
            "\n",
            "After 4000 training steps,w1 is\n",
            "[[0.94564295]\n",
            " [1.0458139 ]] \n",
            "\n",
            "After 4500 training steps,w1 is\n",
            "[[0.9563044]\n",
            " [1.0367833]] \n",
            "\n",
            "After 5000 training steps,w1 is\n",
            "[[0.9649721]\n",
            " [1.0292872]] \n",
            "\n",
            "After 5500 training steps,w1 is\n",
            "[[0.97205573]\n",
            " [1.0231088 ]] \n",
            "\n",
            "After 6000 training steps,w1 is\n",
            "[[0.97785723]\n",
            " [1.018031  ]] \n",
            "\n",
            "After 6500 training steps,w1 is\n",
            "[[0.982613 ]\n",
            " [1.0138624]] \n",
            "\n",
            "After 7000 training steps,w1 is\n",
            "[[0.986513 ]\n",
            " [1.0104423]] \n",
            "\n",
            "After 7500 training steps,w1 is\n",
            "[[0.98971164]\n",
            " [1.0076357 ]] \n",
            "\n",
            "After 8000 training steps,w1 is\n",
            "[[0.99233526]\n",
            " [1.0053347 ]] \n",
            "\n",
            "After 8500 training steps,w1 is\n",
            "[[0.9944871]\n",
            " [1.0034462]] \n",
            "\n",
            "After 9000 training steps,w1 is\n",
            "[[0.9962523]\n",
            " [1.001897 ]] \n",
            "\n",
            "After 9500 training steps,w1 is\n",
            "[[0.9976998]\n",
            " [1.0006279]] \n",
            "\n",
            "After 10000 training steps,w1 is\n",
            "[[0.99888706]\n",
            " [0.9995862 ]] \n",
            "\n",
            "After 10500 training steps,w1 is\n",
            "[[0.99986154]\n",
            " [0.99873143]] \n",
            "\n",
            "After 11000 training steps,w1 is\n",
            "[[1.0006588 ]\n",
            " [0.99803025]] \n",
            "\n",
            "After 11500 training steps,w1 is\n",
            "[[1.0013133 ]\n",
            " [0.99745494]] \n",
            "\n",
            "After 12000 training steps,w1 is\n",
            "[[1.0018501]\n",
            " [0.9969841]] \n",
            "\n",
            "After 12500 training steps,w1 is\n",
            "[[1.002292  ]\n",
            " [0.99659795]] \n",
            "\n",
            "After 13000 training steps,w1 is\n",
            "[[1.0026562 ]\n",
            " [0.99628043]] \n",
            "\n",
            "After 13500 training steps,w1 is\n",
            "[[1.0029525]\n",
            " [0.9960221]] \n",
            "\n",
            "After 14000 training steps,w1 is\n",
            "[[1.0031909]\n",
            " [0.9958076]] \n",
            "\n",
            "After 14500 training steps,w1 is\n",
            "[[1.0033832 ]\n",
            " [0.99563617]] \n",
            "\n",
            "Final w1 is: [[1.0035577]\n",
            " [0.9954898]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 如果也测多了，损失的是成本，预测少了，损失的是利润\n",
        "# 如果利润不等于成本，那mes产生的loss无法实现利益最大化\n",
        "# 此时使用自定义损失函数\n",
        "# 这时假设成本1元，利润99\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "SEED=23455\n",
        "COST=1\n",
        "PROFIT=99\n",
        "rdm=np.random.RandomState(seed=SEED)  #生成[0,1)之间的随机数\n",
        "x=rdm.rand(32,2)#生成32行两列的随机数，也就是x1和x2\n",
        "y_=[[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in x]  # .rand函数生成噪声[0,1)/10=[0,0.1);[0,0.1)-0.05=[-0.05,0.05)\n",
        "x=tf.cast(x, dtype=tf.float32)\n",
        "w1=tf.Variable(tf.random.normal([2,1], stddev=1,seed=1))#随机初始化w1\n",
        "epoch=15000\n",
        "lr=0.002\n",
        "for epoch in range(epoch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y=tf.matmul(x,w1)#前向传播计算结果\n",
        "    loss=tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*COST,(y_-y)*PROFIT))\n",
        "    #预测的y<y_了会损失利润，大于了会损失成本（cost）\n",
        "    # tf.greater(a,b)比较ab的大小，a大于b输出true反之输出false。\n",
        "    # tf。where询问，y大于y_吗。如果成立，就输出第一个，如果不成立，输出第二个。最后对所有损失求和。\n",
        "  grads=tape.gradient(loss,w1)\n",
        "  w1.assign_sub(lr*grads)\n",
        "  if epoch%500==0:\n",
        "    print(\"After %d training steps,w1 is\"%(epoch))\n",
        "    print(w1.numpy(),\"\\n\")\n",
        "print(\"Final w1 is:\",w1.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilrkg_UGatK9",
        "outputId": "80545b00-dfaa-451f-e607-3a76d07a1035"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 0 training steps,w1 is\n",
            "[[3.0493035]\n",
            " [3.0913029]] \n",
            "\n",
            "After 500 training steps,w1 is\n",
            "[[1.1228163]\n",
            " [1.0298307]] \n",
            "\n",
            "After 1000 training steps,w1 is\n",
            "[[1.1131963]\n",
            " [1.0611144]] \n",
            "\n",
            "After 1500 training steps,w1 is\n",
            "[[1.1035764]\n",
            " [1.0923984]] \n",
            "\n",
            "After 2000 training steps,w1 is\n",
            "[[1.1525896]\n",
            " [1.1349752]] \n",
            "\n",
            "After 2500 training steps,w1 is\n",
            "[[1.1429697]\n",
            " [1.1662592]] \n",
            "\n",
            "After 3000 training steps,w1 is\n",
            "[[1.171165]\n",
            " [1.032366]] \n",
            "\n",
            "After 3500 training steps,w1 is\n",
            "[[1.1615452]\n",
            " [1.06365  ]] \n",
            "\n",
            "After 4000 training steps,w1 is\n",
            "[[1.1519252]\n",
            " [1.0949339]] \n",
            "\n",
            "After 4500 training steps,w1 is\n",
            "[[1.1423051]\n",
            " [1.1262176]] \n",
            "\n",
            "After 5000 training steps,w1 is\n",
            "[[1.1326848]\n",
            " [1.1575011]] \n",
            "\n",
            "After 5500 training steps,w1 is\n",
            "[[1.16088  ]\n",
            " [1.0236079]] \n",
            "\n",
            "After 6000 training steps,w1 is\n",
            "[[1.1512599]\n",
            " [1.0548916]] \n",
            "\n",
            "After 6500 training steps,w1 is\n",
            "[[1.1416402]\n",
            " [1.0861757]] \n",
            "\n",
            "After 7000 training steps,w1 is\n",
            "[[1.13202  ]\n",
            " [1.1174593]] \n",
            "\n",
            "After 7500 training steps,w1 is\n",
            "[[1.1810334]\n",
            " [1.1600363]] \n",
            "\n",
            "After 8000 training steps,w1 is\n",
            "[[1.1505953]\n",
            " [1.01485  ]] \n",
            "\n",
            "After 8500 training steps,w1 is\n",
            "[[1.1409754]\n",
            " [1.0461339]] \n",
            "\n",
            "After 9000 training steps,w1 is\n",
            "[[1.131355 ]\n",
            " [1.0774174]] \n",
            "\n",
            "After 9500 training steps,w1 is\n",
            "[[1.1217349]\n",
            " [1.108701 ]] \n",
            "\n",
            "After 10000 training steps,w1 is\n",
            "[[1.1707482]\n",
            " [1.151278 ]] \n",
            "\n",
            "After 10500 training steps,w1 is\n",
            "[[1.1611283]\n",
            " [1.1825619]] \n",
            "\n",
            "After 11000 training steps,w1 is\n",
            "[[1.1306902]\n",
            " [1.0373756]] \n",
            "\n",
            "After 11500 training steps,w1 is\n",
            "[[1.1210703]\n",
            " [1.0686594]] \n",
            "\n",
            "After 12000 training steps,w1 is\n",
            "[[1.1114503]\n",
            " [1.0999433]] \n",
            "\n",
            "After 12500 training steps,w1 is\n",
            "[[1.1604637]\n",
            " [1.1425203]] \n",
            "\n",
            "After 13000 training steps,w1 is\n",
            "[[1.1508437]\n",
            " [1.1738042]] \n",
            "\n",
            "After 13500 training steps,w1 is\n",
            "[[1.1204053]\n",
            " [1.0286175]] \n",
            "\n",
            "After 14000 training steps,w1 is\n",
            "[[1.1107852]\n",
            " [1.0599012]] \n",
            "\n",
            "After 14500 training steps,w1 is\n",
            "[[1.1011653]\n",
            " [1.0911851]] \n",
            "\n",
            "Final w1 is: [[1.1612567]\n",
            " [1.1565249]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gY6TeHNOdQ5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}